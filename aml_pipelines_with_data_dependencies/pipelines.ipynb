{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Azure Machine Learning Pipelines with Data Dependency\n",
        "\n",
        "We will see how we can build a pipeline with implicit data dependency."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace, Experiment, Datastore\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget, ComputeInstance\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n",
        "\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.pipeline.core import Pipeline, PipelineData\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core.graph import PipelineParameter\n",
        "print(\"Pipeline SDK-specific imports completed\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.51.0\nPipeline SDK-specific imports completed\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1712919933240
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline param\n",
        "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919933471
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# source directory\n",
        "source_directory = '_run_train'\n",
        "    \n",
        "print('Sample scripts will be created in {} directory.'.format(source_directory))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Sample scripts will be created in _run_train directory.\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919933782
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
        "\n",
        "# Default datastore (Azure blob storage)\n",
        "# def_blob_store = ws.get_default_datastore()\n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ZEMLSWSBI01T\nTAFE-TST-ARG-DS\naustraliaeast\nf242a3b6-370c-4b2e-971d-a0399c53d6f0\nBlobstore's name: workspaceblobstore\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919934271
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference the data uploaded to blob storage using DataReference\n",
        "# Assign the datasource to blob_input_data variable\n",
        "\n",
        "# DataReference(datastore, \n",
        "#               data_reference_name=None, \n",
        "#               path_on_datastore=None, \n",
        "#               mode='mount', \n",
        "#               path_on_compute=None, \n",
        "#               overwrite=False)\n",
        "\n",
        "path_on_datastore = \"titanic/Titanic.csv\"\n",
        "\n",
        "blob_input_data = DataReference(\n",
        "    datastore=def_blob_store,\n",
        "    data_reference_name=\"test_data\",\n",
        "    path_on_datastore=path_on_datastore)\n",
        "print(\"Step 1: DataReference object created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step 1: DataReference object created\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919934540
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the compute target\n",
        "aml_compute_target = \"CPU-MD\"\n",
        "\n",
        "# Try to get the existing compute instance\n",
        "aml_compute = AmlCompute(ws, aml_compute_target)\n",
        "print(\"Step 2: AML Compute target created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step 2: AML Compute target created.\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919934726
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intermediate/Output Data\n",
        "\n",
        "Intermediate data (or output of a Step) is represented by PipelineData object. PipelineData can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define intermediate data using PipelineData\n",
        "# PipelineData(name, \n",
        "#              datastore=None, \n",
        "#              output_name=None, \n",
        "#              output_mode='mount', \n",
        "#              output_path_on_compute=None, \n",
        "#              output_overwrite=None, \n",
        "#              data_type=None, \n",
        "#              is_directory=None)\n",
        "\n",
        "# Naming the intermediate data as processed_data1 and assigning it to the variable processed_data1.\n",
        "processed_data1 = PipelineData(\"processed_data1\",datastore=def_blob_store, is_directory=True)\n",
        "print(\"Step 3: PipelineData object created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step 3: PipelineData object created\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919934923
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Pipelines steps using datasources and intermediate data\n",
        "\n",
        "Machine learning pipelines can have many steps and these steps could use or reuse datasources and intermediate data. Here's how we construct such a pipeline:\n",
        "\n",
        "##### Define a Step that consumes a datasource and produces intermediate data.\n",
        "\n",
        "In this step, we define a step that consumes a datasource and produces intermediate data.\n",
        "\n",
        "Open train.py in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.\n",
        "Specify conda dependencies and a base docker image through a RunConfiguration\n",
        "\n",
        "This step uses a docker image and scikit-learn, use a RunConfiguration to specify these requirements and use when creating the PythonScriptStep.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.runconfig import DockerConfiguration, RunConfiguration, DEFAULT_CPU_IMAGE\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "# create a new runconfig object\n",
        "run_config = RunConfiguration()\n",
        "\n",
        "# enable Docker using the DockerConfiguration object\n",
        "run_config.docker = DockerConfiguration(use_docker=True)\n",
        "\n",
        "# set Docker base image to the default CPU-based image\n",
        "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "\n",
        "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
        "run_config.environment.python.user_managed_dependencies = False\n",
        "\n",
        "# specify CondaDependencies obj\n",
        "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919935166
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define a Step that consumes intermediate data and produces intermediate data\n",
        "\n",
        "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
        "\n",
        "Open extract.py in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step4 consumes the datasource (Datareference) in the previous step\n",
        "# and produces processed_data1\n",
        "trainStep = PythonScriptStep(\n",
        "    script_name=\"train.py\", \n",
        "    arguments=[\"--input_data\", blob_input_data, \n",
        "               \"--output_train\", processed_data1,\n",
        "               \"--reg\", reg_param],\n",
        "    inputs=[blob_input_data],\n",
        "    outputs=[processed_data1],\n",
        "    compute_target=aml_compute, \n",
        "    source_directory=source_directory,\n",
        "    runconfig=run_config\n",
        ")\n",
        "print(\"Step 4: trainStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step 4: trainStep created\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919935353
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define a Step that consumes intermediate data and produces intermediate data\n",
        "\n",
        "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
        "\n",
        "Open extract.py in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step5 to use the intermediate data produced by step4\n",
        "# This step also produces an output processed_data2\n",
        "processed_data2 = PipelineData(\"processed_data2\", datastore=def_blob_store, is_directory=True)\n",
        "source_directory = \"_run_extract\"\n",
        "\n",
        "extractStep = PythonScriptStep(\n",
        "    script_name=\"extract.py\",\n",
        "    arguments=[\"--input_extract\", processed_data1, \"--output_extract\", processed_data2],\n",
        "    inputs=[processed_data1],\n",
        "    outputs=[processed_data2],\n",
        "    compute_target=aml_compute, \n",
        "    source_directory=source_directory)\n",
        "print(\"Step 5: extractStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step 5: extractStep created\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919935547
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define a Step that consumes intermediate data and existing data and produces intermediate data\n",
        "\n",
        "In this step, we define a step that consumes multiple data types and produces intermediate data.\n",
        "\n",
        "This step uses the output generated from the previous step as well as existing data on a DataStore. The location of the existing data is specified using a PipelineParameter and a DataPath. Using a PipelineParameter enables easy modification of the data location when the Pipeline is published and resubmitted.\n",
        "\n",
        "Open compare.py in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference the data uploaded to blob storage using a PipelineParameter and a DataPath\n",
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
        "\n",
        "datapath = DataPath(datastore=def_blob_store, path_on_datastore='titanic/Titanic.csv')\n",
        "datapath_param = PipelineParameter(name=\"compare_data\", default_value=datapath)\n",
        "data_parameter1 = (datapath_param, DataPathComputeBinding(mode='mount'))"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919935759
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now define the compare step which takes two inputs and produces an output\n",
        "processed_data3 = PipelineData(\"processed_data3\", datastore=def_blob_store, is_directory=True)\n",
        "source_directory = \"_run_compare\"\n",
        "\n",
        "compareStep = PythonScriptStep(\n",
        "    script_name=\"compare.py\",\n",
        "    arguments=[\"--compare_data1\", data_parameter1, \"--compare_data2\", processed_data2, \"--output_compare\", processed_data3],\n",
        "    inputs=[data_parameter1, processed_data2],\n",
        "    outputs=[processed_data3],    \n",
        "    compute_target=aml_compute, \n",
        "    source_directory=source_directory)\n",
        "print(\"Step 6: compareStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Step 6: compareStep created\n"
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919935989
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline1 = Pipeline(workspace=ws, steps=[compareStep])\n",
        "print (\"Pipeline is built\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919936202
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run1 = Experiment(ws, 'Data_dependency_sample').submit(pipeline1)\n",
        "print(\"Pipeline is submitted for execution\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919937486
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RunDetails(pipeline_run1).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80be40893ac84b638995f1e5e64c6511"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/986ac013-3ece-4c69-8e97-8778ad8b589a?wsid=/subscriptions/f242a3b6-370c-4b2e-971d-a0399c53d6f0/resourcegroups/TAFE-TST-ARG-DS/workspaces/ZEMLSWSBI01T&tid=66269612-ce5a-482e-8ea4-fdf6b5ad33af\", \"run_id\": \"986ac013-3ece-4c69-8e97-8778ad8b589a\", \"run_properties\": {\"run_id\": \"986ac013-3ece-4c69-8e97-8778ad8b589a\", \"created_utc\": \"2024-04-12T11:05:36.962856Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{\\\"reg_rate\\\":\\\"0.01\\\"}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.continue_on_failed_optional_input\": \"True\", \"azureml.pipelineComponent\": \"pipelinerun\", \"azureml.pipelines.stages\": \"{\\\"Initialization\\\":null,\\\"Execution\\\":{\\\"StartTime\\\":\\\"2024-04-12T11:05:37.8577676+00:00\\\",\\\"EndTime\\\":\\\"2024-04-12T11:06:04.3932573+00:00\\\",\\\"Status\\\":\\\"Finished\\\"}}\"}, \"tags\": {}, \"end_time_utc\": \"2024-04-12T11:06:04.50872Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://zemlsstorbi01t.blob.core.windows.net/azureml/ExperimentRun/dcid.986ac013-3ece-4c69-8e97-8778ad8b589a/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=Fg6oKJeFIs41j8nCvyxWR06YtDwgOR2W8N0CH2XVD6g%3D&skoid=e977d5ee-0c02-4489-924b-b957d28af788&sktid=66269612-ce5a-482e-8ea4-fdf6b5ad33af&skt=2024-04-12T10%3A53%3A25Z&ske=2024-04-13T19%3A03%3A25Z&sks=b&skv=2019-07-07&st=2024-04-12T10%3A55%3A59Z&se=2024-04-12T19%3A05%3A59Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://zemlsstorbi01t.blob.core.windows.net/azureml/ExperimentRun/dcid.986ac013-3ece-4c69-8e97-8778ad8b589a/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=47zyBTJMHIvEd1N8EuZMH%2FnzquaFjUHtD3uxUES4eNE%3D&skoid=e977d5ee-0c02-4489-924b-b957d28af788&sktid=66269612-ce5a-482e-8ea4-fdf6b5ad33af&skt=2024-04-12T10%3A53%3A25Z&ske=2024-04-13T19%3A03%3A25Z&sks=b&skv=2019-07-07&st=2024-04-12T10%3A55%3A59Z&se=2024-04-12T19%3A05%3A59Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://zemlsstorbi01t.blob.core.windows.net/azureml/ExperimentRun/dcid.986ac013-3ece-4c69-8e97-8778ad8b589a/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=XXpe2T03AHh6TDWT1hHLMpyz4Jj9v86JmCYbzszS4XA%3D&skoid=e977d5ee-0c02-4489-924b-b957d28af788&sktid=66269612-ce5a-482e-8ea4-fdf6b5ad33af&skt=2024-04-12T10%3A53%3A25Z&ske=2024-04-13T19%3A03%3A25Z&sks=b&skv=2019-07-07&st=2024-04-12T10%3A55%3A59Z&se=2024-04-12T19%3A05%3A59Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:00:27\", \"run_number\": \"1712919936\", \"run_queued_details\": {\"status\": \"Finished\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"9fc5846b-9797-4d02-9a9e-98e3cad9bdf0\", \"name\": \"compare.py\", \"status\": \"Finished\", \"start_time\": \"2024-04-12T11:05:48.307615Z\", \"created_time\": \"2024-04-12T11:05:41.353505Z\", \"end_time\": \"2024-04-12T11:06:03.309315Z\", \"duration\": \"0:00:21\", \"run_number\": 1712919941, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2024-04-12T11:05:41.353505Z\", \"is_reused\": \"\"}, {\"run_id\": \"214ae63f-0ad7-4859-9a83-40cff02343ab\", \"name\": \"extract.py\", \"status\": \"Finished\", \"start_time\": \"2024-04-12T11:05:39.370582Z\", \"created_time\": \"2024-04-12T11:05:39.216576Z\", \"end_time\": \"2024-04-12T11:05:39.370582Z\", \"duration\": \"0:00:00\", \"run_number\": 1712919939, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2024-04-12T11:05:39.216576Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"abd2f1af-b4f1-4f44-8b31-c3e7986400a7\", \"name\": \"train_params.py\", \"status\": \"Finished\", \"start_time\": \"2024-04-12T11:05:38.249991Z\", \"created_time\": \"2024-04-12T11:05:38.11536Z\", \"end_time\": \"2024-04-12T11:05:38.249991Z\", \"duration\": \"0:00:00\", \"run_number\": 1712919938, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2024-04-12T11:05:38.11536Z\", \"is_reused\": \"Yes\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2024-04-12 11:05:38Z] Completing processing run id abd2f1af-b4f1-4f44-8b31-c3e7986400a7.\\n[2024-04-12 11:05:39Z] Completing processing run id 214ae63f-0ad7-4859-9a83-40cff02343ab.\\n[2024-04-12 11:05:40Z] Submitting 1 runs, first five are: acd1b227:9fc5846b-9797-4d02-9a9e-98e3cad9bdf0\\n[2024-04-12 11:06:03Z] Completing processing run id 9fc5846b-9797-4d02-9a9e-98e3cad9bdf0.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"57c247b3\": {\"node_id\": \"57c247b3\", \"name\": \"Data source for data path parameter compare_data\"}, \"27b13afa\": {\"node_id\": \"27b13afa\", \"name\": \"test_data\"}}, \"module_nodes\": {\"acd1b227\": {\"node_id\": \"acd1b227\", \"name\": \"compare.py\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"9fc5846b-9797-4d02-9a9e-98e3cad9bdf0\"}, \"dcc42c80\": {\"node_id\": \"dcc42c80\", \"name\": \"extract.py\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"214ae63f-0ad7-4859-9a83-40cff02343ab\"}, \"4ac45ac0\": {\"node_id\": \"4ac45ac0\", \"name\": \"train_params.py\", \"status\": \"Finished\", \"_is_reused\": true, \"run_id\": \"abd2f1af-b4f1-4f44-8b31-c3e7986400a7\"}}, \"edges\": [{\"source_node_id\": \"57c247b3\", \"source_node_name\": \"Data source for data path parameter compare_data\", \"source_name\": \"data\", \"target_name\": \"workspaceblobstore_16deb4c7\", \"dst_node_id\": \"acd1b227\", \"dst_node_name\": \"compare.py\"}, {\"source_node_id\": \"dcc42c80\", \"source_node_name\": \"extract.py\", \"source_name\": \"processed_data2\", \"target_name\": \"workspaceblobstore_16deb4c7\", \"dst_node_id\": \"acd1b227\", \"dst_node_name\": \"compare.py\"}, {\"source_node_id\": \"4ac45ac0\", \"source_node_name\": \"train_params.py\", \"source_name\": \"processed_data1\", \"target_name\": \"processed_data1\", \"dst_node_id\": \"dcc42c80\", \"dst_node_name\": \"extract.py\"}, {\"source_node_id\": \"27b13afa\", \"source_node_name\": \"test_data\", \"source_name\": \"data\", \"target_name\": \"test_data\", \"dst_node_id\": \"4ac45ac0\", \"dst_node_name\": \"train_params.py\"}], \"child_runs\": [{\"run_id\": \"9fc5846b-9797-4d02-9a9e-98e3cad9bdf0\", \"name\": \"compare.py\", \"status\": \"Finished\", \"start_time\": \"2024-04-12T11:05:48.307615Z\", \"created_time\": \"2024-04-12T11:05:41.353505Z\", \"end_time\": \"2024-04-12T11:06:03.309315Z\", \"duration\": \"0:00:21\", \"run_number\": 1712919941, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2024-04-12T11:05:41.353505Z\", \"is_reused\": \"\"}, {\"run_id\": \"214ae63f-0ad7-4859-9a83-40cff02343ab\", \"name\": \"extract.py\", \"status\": \"Finished\", \"start_time\": \"2024-04-12T11:05:39.370582Z\", \"created_time\": \"2024-04-12T11:05:39.216576Z\", \"end_time\": \"2024-04-12T11:05:39.370582Z\", \"duration\": \"0:00:00\", \"run_number\": 1712919939, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2024-04-12T11:05:39.216576Z\", \"is_reused\": \"Yes\"}, {\"run_id\": \"abd2f1af-b4f1-4f44-8b31-c3e7986400a7\", \"name\": \"train_params.py\", \"status\": \"Finished\", \"start_time\": \"2024-04-12T11:05:38.249991Z\", \"created_time\": \"2024-04-12T11:05:38.11536Z\", \"end_time\": \"2024-04-12T11:05:38.249991Z\", \"duration\": \"0:00:00\", \"run_number\": 1712919938, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2024-04-12T11:05:38.11536Z\", \"is_reused\": \"Yes\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.51.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919937753
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run1.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919966871
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Steps\n",
        "for step in pipeline_run1.get_steps():\n",
        "    print(\"Outputs of step \" + step.name)\n",
        "    \n",
        "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
        "    output_dict = step.get_outputs()\n",
        "    \n",
        "    for name, output in output_dict.items():\n",
        "        \n",
        "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
        "        print(\"\\tname: \" + name)\n",
        "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
        "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919968569
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the step runs by name 'train.py'\n",
        "train_step = pipeline_run1.find_step_run('train.py')\n",
        "\n",
        "if train_step:\n",
        "    train_step_obj = train_step[0] # since we have only one step by name 'train.py'\n",
        "    train_step_obj.get_output_data('processed_data1').download(\"./outputs\") # download the output to current directory"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919969305
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Publish the pipeline\n",
        "\n",
        "After we create a pipeline, we can publish it to create a REST endpoint through which the pipeline can be run on demand."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the most recent run of the pipeline\n",
        "pipeline_experiment = ws.experiments.get('Data_dependency_sample')\n",
        "run = list(pipeline_experiment.get_runs())[0]\n",
        "\n",
        "# Publish the pipeline from the run\n",
        "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
        "                                          description='Model training pipeline',\n",
        "                                          version='1.0')"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919972747
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "URL of the endpoint will look like this"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rest_endpoint = published_pipeline.endpoint\n",
        "print(rest_endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919972855
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Using a published pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import requests\n",
        "\n",
        "auth = InteractiveLoginAuthentication()\n",
        "auth_header = auth.get_authentication_header()\n",
        "\n",
        "response = requests.post(rest_endpoint,\n",
        "                         headers=auth_header,\n",
        "                         json={\"ExperimentName\": \"Data_dependency_sample\",\n",
        "                               \"ParameterAssignments\": {\"reg_rate\": 0.1}})\n",
        "run_id = response.json()[\"Id\"]\n",
        "print(run_id)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919973587
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "\n",
        "##### **Additional/Optional** - Scheduling pipelines\n",
        "\n",
        "Schedule operations require id of a published pipeline. You can get all published pipelines and do Schedule operations on them, or if you already know the id of the published pipeline, you can use it directly as well."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
        "\n",
        "# set up a specific recurrance interval\n",
        "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=2, hours=[22], minutes=[30]) # Runs every other day at 10:30pm\n",
        "\n",
        "print(\"Published pipeline id to be used for Schedule operations: {}\".format(published_pipeline.id))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919973778
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = Schedule.create(ws, name='Scheduled Training',\n",
        "                                description='trains model every other day at 10:30 PM',\n",
        "                                pipeline_id=published_pipeline.id,\n",
        "                                experiment_name='Training_Pipeline',\n",
        "                                recurrence=recurrence)\n",
        "\n",
        "print(\"Created schedule with id: {}\".format(schedule.id))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712919974009
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Triggering a pipeline run on data changes\n",
        "\n",
        "We can also schedule a pipeline run depending on changes in the data. To schedule a pipeline to run whenever data changes, you must create a Schedule that monitors a specified path on a datastore."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore\n",
        "from azureml.pipeline.core import Schedule\n",
        "\n",
        "training_datastore = Datastore(workspace=ws, name=def_blob_store.name)\n",
        "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
        "                                    description='trains model on data change',\n",
        "                                    pipeline_id=published_pipeline.id,\n",
        "                                    experiment_name='Training_Pipeline',\n",
        "                                    datastore=training_datastore,\n",
        "                                    path_on_datastore=path_on_datastore)"
      ],
      "outputs": [],
      "execution_count": 54,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1712920042470
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {
          "undefined": {
            "model_name": "ShowPipelineRunsModel",
            "model_module": "azureml_widgets",
            "model_module_version": "2.0.0",
            "state": {
              "_view_module_version": "2.0.0",
              "_model_name": "WidgetModel",
              "_model_module": "@jupyter-widgets/base",
              "msg": "Model class 'ShowPipelineRunsModel' from module 'azureml_widgets' is loaded but can not be instantiated",
              "tooltip": null,
              "_view_name": "ErrorWidgetView",
              "tabbable": null,
              "_view_module": "@jupyter-widgets/base",
              "error": {},
              "_dom_classes": [],
              "_view_count": null,
              "_model_module_version": "2.0.0"
            }
          },
          "095c9c2e37a6449380e02714eb1fb68a": {
            "model_name": "LayoutModel",
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "1.2.0",
            "state": {
              "_view_module_version": "1.2.0",
              "_model_name": "LayoutModel",
              "grid_row": null,
              "_model_module": "@jupyter-widgets/base",
              "overflow": null,
              "max_height": null,
              "display": null,
              "border_top": null,
              "grid_auto_flow": null,
              "grid_template_rows": null,
              "align_self": null,
              "grid_auto_columns": null,
              "width": null,
              "grid_area": null,
              "align_items": null,
              "_view_name": "LayoutView",
              "left": null,
              "height": null,
              "_view_module": "@jupyter-widgets/base",
              "border_right": null,
              "object_position": null,
              "justify_content": null,
              "bottom": null,
              "max_width": null,
              "border": null,
              "margin": null,
              "order": null,
              "grid_column": null,
              "grid_auto_rows": null,
              "padding": null,
              "grid_template_columns": null,
              "justify_items": null,
              "object_fit": null,
              "visibility": null,
              "_view_count": null,
              "flex_flow": null,
              "min_height": null,
              "top": null,
              "min_width": null,
              "flex": null,
              "border_left": null,
              "_model_module_version": "1.2.0",
              "grid_template_areas": null,
              "overflow_x": null,
              "right": null,
              "overflow_y": null,
              "grid_gap": null,
              "border_bottom": null,
              "align_content": null
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
